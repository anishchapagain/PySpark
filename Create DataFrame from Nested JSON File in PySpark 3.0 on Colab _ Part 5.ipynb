{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1513,
     "status": "ok",
     "timestamp": 1599909201926,
     "user": {
      "displayName": "DataMaking",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Ck4eyinapgtobtrErZWzNt_3DfJL867BQ7JA-Q=s64",
      "userId": "16773646856361708024"
     },
     "user_tz": -330
    },
    "id": "bPyOO2j9eNtt",
    "outputId": "6cfec856-3e00-479d-b2c2-237a058fb80f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/16 15:48:55 WARN Utils: Your hostname, Anishs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.90.85 instead (on interface en0)\n",
      "21/12/16 15:48:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/anishchapagain/Library/Python/3.9/lib/python/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/16 15:48:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/16 15:48:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/16 15:48:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DataFrame\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Create DataFrame\").getOrCreate()\n",
    "print(spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1353,
     "status": "ok",
     "timestamp": 1599909232694,
     "user": {
      "displayName": "DataMaking",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Ck4eyinapgtobtrErZWzNt_3DfJL867BQ7JA-Q=s64",
      "userId": "16773646856361708024"
     },
     "user_tz": -330
    },
    "id": "vEBTnZyjeWwn",
    "outputId": "22911089-d3e3-4263-e567-bcfd58577360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------------------------------------------------------+----+-------------+\n",
      "|CHECK   |COL|DATA                                                    |IFAM|KTM          |\n",
      "+--------+---+--------------------------------------------------------+----+-------------+\n",
      "|{1, TWO}|21 |[{[{k1, v1}, {k2, v2}], 31}, {[{k3, v3}, {k4, v4}], 33}]|EQR |1548176931466|\n",
      "+--------+---+--------------------------------------------------------+----+-------------+\n",
      "\n",
      "root\n",
      " |-- CHECK: struct (nullable = true)\n",
      " |    |-- Check1: long (nullable = true)\n",
      " |    |-- Check2: string (nullable = true)\n",
      " |-- COL: long (nullable = true)\n",
      " |-- DATA: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Crate: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- key: string (nullable = true)\n",
      " |    |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- MLrate: string (nullable = true)\n",
      " |-- IFAM: string (nullable = true)\n",
      " |-- KTM: long (nullable = true)\n",
      "\n",
      "+------+------+---+--------------------------------------------------------+----+-------------+\n",
      "|Check1|Check2|COL|DATA                                                    |IFAM|KTM          |\n",
      "+------+------+---+--------------------------------------------------------+----+-------------+\n",
      "|1     |TWO   |21 |[{[{k1, v1}, {k2, v2}], 31}, {[{k3, v3}, {k4, v4}], 33}]|EQR |1548176931466|\n",
      "+------+------+---+--------------------------------------------------------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested_json_file_path = \"sample_nested_json_file.json\"\n",
    "\n",
    "df = spark.read.json(path=nested_json_file_path, multiLine=True)\n",
    "\n",
    "df.show(10, False)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.select(['CHECK.Check1', 'CHECK.Check2', 'COL', 'DATA', 'IFAM', 'KTM']).show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1815,
     "status": "ok",
     "timestamp": 1599909767874,
     "user": {
      "displayName": "DataMaking",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Ck4eyinapgtobtrErZWzNt_3DfJL867BQ7JA-Q=s64",
      "userId": "16773646856361708024"
     },
     "user_tz": -330
    },
    "id": "6VJoCqUbuotD",
    "outputId": "77c96725-3b77-4a7e-ed97-1bc7e62a7ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Nested JSON File ... \n",
      "Outside isinstance loop: CHECK\n",
      "Inside isinstance loop of StructType: CHECK\n",
      "Outside isinstance loop: COL\n",
      "Outside isinstance loop: DATA\n",
      "Inside isinstance loop of ArrayType: DATA\n",
      "Outside isinstance loop: IFAM\n",
      "Outside isinstance loop: KTM\n",
      "+------------+------------+---+--------------------------+----+-------------+\n",
      "|CHECK_Check1|CHECK_Check2|COL|DATA                      |IFAM|KTM          |\n",
      "+------------+------------+---+--------------------------+----+-------------+\n",
      "|1           |TWO         |21 |[[[k1, v1], [k2, v2]], 31]|EQR |1548176931466|\n",
      "|1           |TWO         |21 |[[[k3, v3], [k4, v4]], 33]|EQR |1548176931466|\n",
      "+------------+------------+---+--------------------------+----+-------------+\n",
      "\n",
      "Reading Nested JSON File ... \n",
      "Outside isinstance loop: CHECK_Check1\n",
      "Outside isinstance loop: CHECK_Check2\n",
      "Outside isinstance loop: COL\n",
      "Outside isinstance loop: DATA\n",
      "Inside isinstance loop of StructType: DATA\n",
      "Outside isinstance loop: IFAM\n",
      "Outside isinstance loop: KTM\n",
      "+------------+------------+---+--------------------+-----------+----+-------------+\n",
      "|CHECK_Check1|CHECK_Check2|COL|DATA_Crate          |DATA_MLrate|IFAM|KTM          |\n",
      "+------------+------------+---+--------------------+-----------+----+-------------+\n",
      "|1           |TWO         |21 |[[k1, v1], [k2, v2]]|31         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |[[k3, v3], [k4, v4]]|33         |EQR |1548176931466|\n",
      "+------------+------------+---+--------------------+-----------+----+-------------+\n",
      "\n",
      "Reading Nested JSON File ... \n",
      "Outside isinstance loop: CHECK_Check1\n",
      "Outside isinstance loop: CHECK_Check2\n",
      "Outside isinstance loop: COL\n",
      "Outside isinstance loop: DATA_Crate\n",
      "Inside isinstance loop of ArrayType: DATA_Crate\n",
      "Outside isinstance loop: DATA_MLrate\n",
      "Outside isinstance loop: IFAM\n",
      "Outside isinstance loop: KTM\n",
      "+------------+------------+---+----------+-----------+----+-------------+\n",
      "|CHECK_Check1|CHECK_Check2|COL|DATA_Crate|DATA_MLrate|IFAM|KTM          |\n",
      "+------------+------------+---+----------+-----------+----+-------------+\n",
      "|1           |TWO         |21 |[k1, v1]  |31         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |[k2, v2]  |31         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |[k3, v3]  |33         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |[k4, v4]  |33         |EQR |1548176931466|\n",
      "+------------+------------+---+----------+-----------+----+-------------+\n",
      "\n",
      "Reading Nested JSON File ... \n",
      "Outside isinstance loop: CHECK_Check1\n",
      "Outside isinstance loop: CHECK_Check2\n",
      "Outside isinstance loop: COL\n",
      "Outside isinstance loop: DATA_Crate\n",
      "Inside isinstance loop of StructType: DATA_Crate\n",
      "Outside isinstance loop: DATA_MLrate\n",
      "Outside isinstance loop: IFAM\n",
      "Outside isinstance loop: KTM\n",
      "+------------+------------+---+--------------+----------------+-----------+----+-------------+\n",
      "|CHECK_Check1|CHECK_Check2|COL|DATA_Crate_key|DATA_Crate_value|DATA_MLrate|IFAM|KTM          |\n",
      "+------------+------------+---+--------------+----------------+-----------+----+-------------+\n",
      "|1           |TWO         |21 |k1            |v1              |31         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |k2            |v2              |31         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |k3            |v3              |33         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |k4            |v4              |33         |EQR |1548176931466|\n",
      "+------------+------------+---+--------------+----------------+-----------+----+-------------+\n",
      "\n",
      "+------------+------------+---+--------------+----------------+-----------+----+-------------+\n",
      "|CHECK_Check1|CHECK_Check2|COL|DATA_Crate_key|DATA_Crate_value|DATA_MLrate|IFAM|KTM          |\n",
      "+------------+------------+---+--------------+----------------+-----------+----+-------------+\n",
      "|1           |TWO         |21 |k1            |v1              |31         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |k2            |v2              |31         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |k3            |v3              |33         |EQR |1548176931466|\n",
      "|1           |TWO         |21 |k4            |v4              |33         |EQR |1548176931466|\n",
      "+------------+------------+---+--------------+----------------+-----------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def read_nested_json(df):\n",
    "    column_list = []\n",
    "\n",
    "    for column_name in df.schema.names:\n",
    "        print(\"Outside isinstance loop: \" + column_name)\n",
    "        # Checking column type is ArrayType\n",
    "        if isinstance(df.schema[column_name].dataType, ArrayType):\n",
    "            print(\"Inside isinstance loop of ArrayType: \" + column_name)\n",
    "            df = df.withColumn(column_name, explode(column_name).alias(column_name))\n",
    "            column_list.append(column_name)\n",
    "\n",
    "        elif isinstance(df.schema[column_name].dataType, StructType):\n",
    "            print(\"Inside isinstance loop of StructType: \" + column_name)\n",
    "            for field in df.schema[column_name].dataType.fields:\n",
    "                column_list.append(col(column_name + \".\" + field.name).alias(column_name + \"_\" + field.name))\n",
    "        else:\n",
    "            column_list.append(column_name)\n",
    "\n",
    "    # Selecting columns using column_list from dataframe: df\n",
    "    df = df.select(column_list)\n",
    "    return df\n",
    "\n",
    "read_nested_json_flag = True\n",
    "\n",
    "while read_nested_json_flag:\n",
    "  print(\"Reading Nested JSON File ... \")\n",
    "  df = read_nested_json(df)\n",
    "  df.show(100, False)\n",
    "  read_nested_json_flag = False\n",
    "\n",
    "  for column_name in df.schema.names:\n",
    "    if isinstance(df.schema[column_name].dataType, ArrayType):\n",
    "      read_nested_json_flag = True\n",
    "    elif isinstance(df.schema[column_name].dataType, StructType):\n",
    "      read_nested_json_flag = True\n",
    "\n",
    "df.show(100, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQa3KRH681-_"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNknFFPvpvrX1dpqKCqIdES",
   "collapsed_sections": [],
   "name": "Create DataFrame from Nested JSON File in PySpark 3.0 on Colab | Part 5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
